{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb6cf567",
   "metadata": {},
   "source": [
    "# Homework 4 - Recommendation systems and clustering everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e2c080",
   "metadata": {},
   "source": [
    "Behavioral user data is a valuable resource for understanding audience patterns on Netflix, particularly in the context of UK movies. It offers insights into how viewers interact with the popular streaming platform, allowing researchers and data enthusiasts to explore trends, preferences, and patterns in user engagement with Netflix content. Whether you're interested in analyzing viewing habits, content popularity, or user demographics, this information provides a rich source to gain a deeper understanding of Netflix audience behavior in the United Kingdom.\n",
    "\n",
    "Now, you and your team have been hired by Netflix to get to know their users. In other words, you will implement hashing and clustering techniques to extract relevant information and highlights from those users and their behavior inside the platform.\n",
    "\n",
    "Then, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd6c038",
   "metadata": {},
   "source": [
    "## 1. Recommendation system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cc5f06",
   "metadata": {},
   "source": [
    "Implementing a recommendation system is critical for businesses and digital platforms that want to thrive in today's competitive environment. These systems use data-driven personalization to tailor content, products, and services to individual user preferences. The latter improves user engagement, satisfaction, retention, and revenue through increased sales and cross-selling opportunities. In this section, you will attempt to implement a recommendation system by identifying similar users' preferences and recommending movies they watch to the study user.\n",
    "\n",
    "To be more specific, you will implement your version of the LSH algorithm, which will take as input the user's preferred genre of movies, find the most similar users to this user, and recommend the most watched movies by those who are more similar to the user.\n",
    "\n",
    "Data: The data you will be working with can be found here.\n",
    "\n",
    "Looking at the data, you can see that there is data available for each user for the movies the user clicked on. Gather the title and genre of the maximum top 10 movies that each user clicked on regarding the number of clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eea07145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c234c562",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"vodclickstream_uk_movies_03.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a03a6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>datetime</th>\n",
       "      <th>duration</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>release_date</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58773</td>\n",
       "      <td>2017-01-01 01:15:09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Angus, Thongs and Perfect Snogging</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "      <td>2008-07-25</td>\n",
       "      <td>26bd5987e8</td>\n",
       "      <td>1dea19f6fe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58774</td>\n",
       "      <td>2017-01-01 13:56:02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The Curse of Sleeping Beauty</td>\n",
       "      <td>Fantasy, Horror, Mystery, Thriller</td>\n",
       "      <td>2016-06-02</td>\n",
       "      <td>f26ed2675e</td>\n",
       "      <td>544dcbc510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58775</td>\n",
       "      <td>2017-01-01 15:17:47</td>\n",
       "      <td>10530.0</td>\n",
       "      <td>London Has Fallen</td>\n",
       "      <td>Action, Thriller</td>\n",
       "      <td>2016-03-04</td>\n",
       "      <td>f77e500e7a</td>\n",
       "      <td>7cbcc791bf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58776</td>\n",
       "      <td>2017-01-01 16:04:13</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Vendetta</td>\n",
       "      <td>Action, Drama</td>\n",
       "      <td>2015-06-12</td>\n",
       "      <td>c74aec7673</td>\n",
       "      <td>ebf43c36b6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58777</td>\n",
       "      <td>2017-01-01 19:16:37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The SpongeBob SquarePants Movie</td>\n",
       "      <td>Animation, Action, Adventure, Comedy, Family, ...</td>\n",
       "      <td>2004-11-19</td>\n",
       "      <td>a80d6fc2aa</td>\n",
       "      <td>a57c992287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             datetime  duration  \\\n",
       "0       58773  2017-01-01 01:15:09       0.0   \n",
       "1       58774  2017-01-01 13:56:02       0.0   \n",
       "2       58775  2017-01-01 15:17:47   10530.0   \n",
       "3       58776  2017-01-01 16:04:13      49.0   \n",
       "4       58777  2017-01-01 19:16:37       0.0   \n",
       "\n",
       "                                title  \\\n",
       "0  Angus, Thongs and Perfect Snogging   \n",
       "1        The Curse of Sleeping Beauty   \n",
       "2                   London Has Fallen   \n",
       "3                            Vendetta   \n",
       "4     The SpongeBob SquarePants Movie   \n",
       "\n",
       "                                              genres release_date    movie_id  \\\n",
       "0                             Comedy, Drama, Romance   2008-07-25  26bd5987e8   \n",
       "1                 Fantasy, Horror, Mystery, Thriller   2016-06-02  f26ed2675e   \n",
       "2                                   Action, Thriller   2016-03-04  f77e500e7a   \n",
       "3                                      Action, Drama   2015-06-12  c74aec7673   \n",
       "4  Animation, Action, Adventure, Comedy, Family, ...   2004-11-19  a80d6fc2aa   \n",
       "\n",
       "      user_id  \n",
       "0  1dea19f6fe  \n",
       "1  544dcbc510  \n",
       "2  7cbcc791bf  \n",
       "3  ebf43c36b6  \n",
       "4  a57c992287  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "16942649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(671736, 8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5fbcaf",
   "metadata": {},
   "source": [
    "## datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79414599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[\"datetime\"].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e10840b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2017-01-01 00:02:21'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(df[\"datetime\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6df37476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-06-30 23:59:20'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df[\"datetime\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39694933",
   "metadata": {},
   "source": [
    "## duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "37d2e6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[\"duration\"].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4aaa6238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(df[\"duration\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8d117acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18237253.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df[\"duration\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c0a1118",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"duration\"] >= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfd63d9",
   "metadata": {},
   "source": [
    "## title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e9fbbfbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[\"title\"].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d97a2814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[\"title\"]==\" \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e667044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7874"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"title\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968ebaf8",
   "metadata": {},
   "source": [
    "## genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5014cf89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[\"genres\"].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "88b01df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[\"genres\"]==\" \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0795bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"genres\"] != \"NOT AVAILABLE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de0ff06",
   "metadata": {},
   "source": [
    "## release_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2603572d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[\"release_date\"].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2bcc4a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1920-10-01'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(df[\"release_date\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "63b8a298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOT AVAILABLE'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df[\"release_date\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c0d9d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get only the data since Netflix was created\n",
    "df = df[df[\"release_date\"] >= \"2007-01-16\"]\n",
    "df = df[df[\"release_date\"] != \"NOT AVAILABLE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031c1c11",
   "metadata": {},
   "source": [
    "## movie_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7eb4e3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[\"movie_id\"].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6867c985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5442"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"movie_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e52669c",
   "metadata": {},
   "source": [
    "## user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1b0c86cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df[\"user_id\"].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cb18b5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137665"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"user_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "571d1046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>datetime</th>\n",
       "      <th>duration</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>release_date</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58773</td>\n",
       "      <td>2017-01-01 01:15:09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Angus, Thongs and Perfect Snogging</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "      <td>2008-07-25</td>\n",
       "      <td>26bd5987e8</td>\n",
       "      <td>1dea19f6fe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58774</td>\n",
       "      <td>2017-01-01 13:56:02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The Curse of Sleeping Beauty</td>\n",
       "      <td>Fantasy, Horror, Mystery, Thriller</td>\n",
       "      <td>2016-06-02</td>\n",
       "      <td>f26ed2675e</td>\n",
       "      <td>544dcbc510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58775</td>\n",
       "      <td>2017-01-01 15:17:47</td>\n",
       "      <td>10530.0</td>\n",
       "      <td>London Has Fallen</td>\n",
       "      <td>Action, Thriller</td>\n",
       "      <td>2016-03-04</td>\n",
       "      <td>f77e500e7a</td>\n",
       "      <td>7cbcc791bf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58776</td>\n",
       "      <td>2017-01-01 16:04:13</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Vendetta</td>\n",
       "      <td>Action, Drama</td>\n",
       "      <td>2015-06-12</td>\n",
       "      <td>c74aec7673</td>\n",
       "      <td>ebf43c36b6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>58778</td>\n",
       "      <td>2017-01-01 19:21:37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>London Has Fallen</td>\n",
       "      <td>Action, Thriller</td>\n",
       "      <td>2016-03-04</td>\n",
       "      <td>f77e500e7a</td>\n",
       "      <td>c5bf4f3f57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             datetime  duration  \\\n",
       "0       58773  2017-01-01 01:15:09       0.0   \n",
       "1       58774  2017-01-01 13:56:02       0.0   \n",
       "2       58775  2017-01-01 15:17:47   10530.0   \n",
       "3       58776  2017-01-01 16:04:13      49.0   \n",
       "5       58778  2017-01-01 19:21:37       0.0   \n",
       "\n",
       "                                title                              genres  \\\n",
       "0  Angus, Thongs and Perfect Snogging              Comedy, Drama, Romance   \n",
       "1        The Curse of Sleeping Beauty  Fantasy, Horror, Mystery, Thriller   \n",
       "2                   London Has Fallen                    Action, Thriller   \n",
       "3                            Vendetta                       Action, Drama   \n",
       "5                   London Has Fallen                    Action, Thriller   \n",
       "\n",
       "  release_date    movie_id     user_id  \n",
       "0   2008-07-25  26bd5987e8  1dea19f6fe  \n",
       "1   2016-06-02  f26ed2675e  544dcbc510  \n",
       "2   2016-03-04  f77e500e7a  7cbcc791bf  \n",
       "3   2015-06-12  c74aec7673  ebf43c36b6  \n",
       "5   2016-03-04  f77e500e7a  c5bf4f3f57  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b938d",
   "metadata": {},
   "source": [
    "### 1.2 Minhash Signatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b57fc4e",
   "metadata": {},
   "source": [
    "Using the movie genre and user_ids, try to implement your min-hash signatures so that users with similar interests in a genre appear in the same bucket.\n",
    "\n",
    "Important note: You must write your minhash function from scratch. You are not permitted to use any already implemented hash functions. Read the class materials and, if necessary, conduct an internet search. The description of hash functions in the book may be helpful as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ea445d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get for every user the top10 movies.\n",
    "distinct_movie_genre = df[['movie_id', 'genres', \"title\"]].drop_duplicates()\n",
    "clicks = df.groupby(['user_id', 'movie_id']).size().reset_index(name='Number_of_clicks')\n",
    "movie_genre = clicks.sort_values(by='Number_of_clicks', ascending=False).groupby('user_id').head(10).reset_index(drop=True)\n",
    "movie_genre = pd.merge(movie_genre, distinct_movie_genre, on = \"movie_id\", how = \"left\")\n",
    "movie_genre[\"genres\"] = movie_genre[\"genres\"].str.split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6d9744ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>Number_of_clicks</th>\n",
       "      <th>genres</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7cdfd0e14a</td>\n",
       "      <td>40bccd3001</td>\n",
       "      <td>88</td>\n",
       "      <td>[Drama, Fantasy, Romance]</td>\n",
       "      <td>Twilight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e06f0be797</td>\n",
       "      <td>3f3b34e56f</td>\n",
       "      <td>54</td>\n",
       "      <td>[Action, Comedy, Crime, Thriller]</td>\n",
       "      <td>Rush Hour 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59416738c3</td>\n",
       "      <td>cbdf9820bc</td>\n",
       "      <td>54</td>\n",
       "      <td>[Comedy, Romance]</td>\n",
       "      <td>The Ex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49d091aa63</td>\n",
       "      <td>b8a2658c23</td>\n",
       "      <td>48</td>\n",
       "      <td>[Comedy, Romance, Sport]</td>\n",
       "      <td>Chalet Girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3675d9ba4a</td>\n",
       "      <td>948f2b5bf6</td>\n",
       "      <td>42</td>\n",
       "      <td>[Drama, Romance, Sci-Fi, Thriller]</td>\n",
       "      <td>Passengers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id    movie_id  Number_of_clicks  \\\n",
       "0  7cdfd0e14a  40bccd3001                88   \n",
       "1  e06f0be797  3f3b34e56f                54   \n",
       "2  59416738c3  cbdf9820bc                54   \n",
       "3  49d091aa63  b8a2658c23                48   \n",
       "4  3675d9ba4a  948f2b5bf6                42   \n",
       "\n",
       "                               genres        title  \n",
       "0           [Drama, Fantasy, Romance]     Twilight  \n",
       "1   [Action, Comedy, Crime, Thriller]  Rush Hour 3  \n",
       "2                   [Comedy, Romance]       The Ex  \n",
       "3            [Comedy, Romance, Sport]  Chalet Girl  \n",
       "4  [Drama, Romance, Sci-Fi, Thriller]   Passengers  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_genre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "63da18ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate  slist of uniques genres\n",
    "list_of_genres = list(movie_genre[\"genres\"])\n",
    "unique_genres = set(genre for genres in list_of_genres for genre in genres)\n",
    "unique_genres_list = list(unique_genres)\n",
    "genre_dict = {genre: i for i, genre in enumerate(sorted(unique_genres_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ea01ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genrate a list of unique users\n",
    "users = movie_genre[\"user_id\"].unique()\n",
    "users_dict = {user: i for i, user in enumerate(sorted(users))}\n",
    "inverted_users_dict = {value: key for key, value in users_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "377b359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_representation_users_genres(users_dict, genre_dict, movie_genre):\n",
    "    \"\"\"\n",
    "    Return the matrix representation, with cols as users and rows as genres. 1 will mean that a user has in common this genre, 0 otherwise.\n",
    "    \"\"\"\n",
    "    df_shape = movie_genre.shape[0]\n",
    "    rows = len(genre_dict)\n",
    "    cols = len(users_dict)\n",
    "    matrix_representation = np.zeros((rows, cols), dtype = int)\n",
    "    \n",
    "    for i in range(df_shape):    \n",
    "        user = movie_genre.iloc[i][0]\n",
    "        genres = movie_genre.iloc[i][3]\n",
    "        for genre in genres:\n",
    "            matrix_representation[genre_dict[genre], users_dict[user]] = 1\n",
    "    return matrix_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4b46e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signature_matrix_minhash(n_hashes, hash_function, matrix_representation):\n",
    "    \"\"\"\n",
    "    Compute minhash signature matrix with n_hashes hash functions genrated randomly.\n",
    "    \"\"\"\n",
    "    np.random.seed(41)\n",
    "    cols = len(matrix_representation[0])\n",
    "    signature_matrix = np.full((n_hashes, cols), np.inf)\n",
    "    a_b = [(round(np.random.uniform(1, 999)), round(np.random.uniform(1, 999))) for _ in range(n_hashes)]\n",
    "    for r in range(len(matrix_representation)):\n",
    "\n",
    "        hashes = [hash_function(a_b[i][0], r, a_b[i][1]) for i in range(n_hashes)]\n",
    "\n",
    "        cols_with_one = list(np.nonzero(matrix_representation[r])[0])\n",
    "\n",
    "        for col in cols_with_one:\n",
    "            for h in range(n_hashes):\n",
    "                if signature_matrix[h, col] > hashes[h]:\n",
    "                    signature_matrix[h, col] = hashes[h]\n",
    "    return signature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b2d33a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_representation = matrix_representation_users_genres(users_dict, genre_dict, movie_genre)\n",
    "n_hashes = 20\n",
    "hash_function = lambda a, x, b : (a * x + b) % 31\n",
    "signature_matrix = signature_matrix_minhash(n_hashes, hash_function, matrix_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "39956969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  8.,  6., ...,  6.,  0.,  3.],\n",
       "       [ 1.,  5.,  2., ...,  2.,  2.,  5.],\n",
       "       [ 0., 10., 10., ..., 13.,  4.,  4.],\n",
       "       ...,\n",
       "       [ 1., 10., 10., ..., 10.,  0.,  2.],\n",
       "       [ 0.,  2.,  2., ...,  2.,  0.,  4.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  3.,  0.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b74a8",
   "metadata": {},
   "source": [
    "### 1.3 Locality-Sensitive Hashing (LSH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561ffd68",
   "metadata": {},
   "source": [
    "Now that your buckets are ready, it's time to ask a few queries. We will provide you with some user_ids and ask you to recommend at most five movies to the user to watch based on the movies clicked by similar users.\n",
    "\n",
    "To recommend at most five movies given a user_id, use the following procedure:\n",
    "\n",
    "1. Identify the two most similar users to this user.\n",
    "2. If these two users have any movies in common, recommend those movies based on the total number of clicks by these users.\n",
    "3. If there are no more common movies, try to propose the most clicked movies by the most similar user first, followed by the other user.\n",
    "\n",
    "Note: At the end of the process, we expect to see at most five movies recommended to the user.\n",
    "\n",
    "Example: assume you've identified user A and B as the most similar users to a single user, and we have the following records on these users:\n",
    "\n",
    "- User A with 80% similarity\n",
    "- User B with 50% similarity\n",
    "\n",
    "| user | movie title              | #clicks |\n",
    "|------|--------------------------|---------|\n",
    "| A    | Wild Child               | 20      |\n",
    "| A    | Innocence                | 10      |\n",
    "| A    | Coin Heist               | 2       |\n",
    "| B    | Innocence                | 30      |\n",
    "| B    | Coin Heist               | 15      |\n",
    "| B    | Before I Fall            | 30      |\n",
    "| B    | Beyond Skyline           | 8       |\n",
    "| B    | The Amazing Spider-Man   | 5       |\n",
    "\n",
    "- **Recommended Movies in Order:**\n",
    "   - Innocence\n",
    "   - Coin Heist\n",
    "   - Wild Child\n",
    "   - Before I Fall\n",
    "   - Beyond Skyline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4e7efc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashing_function(bucket):\n",
    "    \"\"\"\n",
    "    Hashing of a tuple. Concatenate all the hasgings of the elements of the tuple.\n",
    "    \"\"\"\n",
    "    hashing = \"\"\n",
    "    np.random.seed(41)\n",
    "    \n",
    "    for elm in bucket:\n",
    "        hashing += str((round(np.random.uniform(1, 999))) * int(elm)+ round(np.random.uniform(1, 999)) % 997)\n",
    "    return int(hashing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9c0a407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsh(signature_matrix, rows, inverted_dict):\n",
    "    \"\"\"\n",
    "    Compute lsh algorithm, and return a dictionary of all buckets as key and users as values.\n",
    "    \"\"\"\n",
    "    buckets = {}\n",
    "    signature_matrix = signature_matrix.T\n",
    "    for index, row in enumerate(signature_matrix):\n",
    "        for n in range(0, len(row),rows):\n",
    "            band = row[n:n+rows]\n",
    "            hashed_value = hashing_function(band)\n",
    "            if hashed_value in buckets:\n",
    "                buckets[hashed_value].append(inverted_dict[index])\n",
    "            else:\n",
    "                buckets[hashed_value] = [inverted_dict[index]]\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1181c53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_user(user_id, bucket):\n",
    "    \"\"\"\n",
    "    Return the 2 most common users to a given user_id.\n",
    "    \"\"\"\n",
    "    buckets_user = []\n",
    "    users = []\n",
    "    for bucket in buckets.values():\n",
    "        if user_id in bucket:\n",
    "            buckets_user.append(bucket)\n",
    "    for bucket in buckets_user:\n",
    "        users.append(bucket)\n",
    "    users = list(np.concatenate(users))\n",
    "    counts = Counter(users).most_common()\n",
    "    return (counts[0][0], counts[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8c4c9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_films(common_users, movie_genre):\n",
    "    \"\"\"\n",
    "    Recommend the most similar films to the given users following the instructions of statment.\n",
    "    \"\"\"\n",
    "    user1 = common_users[0]\n",
    "    user2 = common_users[1]\n",
    "    final_df = pd.DataFrame(columns=[\"movie_id\", \"user_id\", \"title\", \"Number_of_clicks\"])\n",
    "    films_to_show = 5\n",
    "\n",
    "    # 1. Movies in common based on number_clicks\n",
    "    df_movies1 = movie_genre[(movie_genre[\"user_id\"] == user1) & (movie_genre[\"user_id\"] == user2)]\n",
    "    df_movies1 = df_movies1.groupby(\"movie_id\")[\"Number_of_clicks\"].sum().reset_index()\n",
    "    df_movies1 = df_movies1.sort_values(by=\"Number_of_clicks\", ascending=False)\n",
    "    if not df_movies1.empty:\n",
    "        final_df = pd.concat([final_df, df_movies1[[\"movie_id\", \"user_id\", \"title\", \"Number_of_clicks\"]]])\n",
    "\n",
    "    # 2. Most clicked movies by the first user\n",
    "    df_movies2 = movie_genre[movie_genre[\"user_id\"] == user1].sort_values(by=\"Number_of_clicks\", ascending=False)\n",
    "    if not df_movies2.empty:\n",
    "        final_df = pd.concat([final_df, df_movies2[[\"movie_id\", \"user_id\", \"title\", \"Number_of_clicks\"]]])\n",
    "\n",
    "    # 3. Most clicked movies by the second user\n",
    "    df_movies3 = movie_genre[movie_genre[\"user_id\"] == user2].sort_values(by=\"Number_of_clicks\", ascending=False)\n",
    "    if not df_movies3.empty:\n",
    "        final_df = pd.concat([final_df, df_movies3[[\"movie_id\", \"user_id\", \"title\", \"Number_of_clicks\"]]])\n",
    "\n",
    "    return final_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "429ad073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>253803</th>\n",
       "      <td>117c9dc515</td>\n",
       "      <td>Set It Up</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253808</th>\n",
       "      <td>7b3d8d5976</td>\n",
       "      <td>Bring It On: Fight to the Finish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253215</th>\n",
       "      <td>f80b7002bb</td>\n",
       "      <td>Anchorman: The Legend Continues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253216</th>\n",
       "      <td>771f79dd7e</td>\n",
       "      <td>The Love Guru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          movie_id                             title\n",
       "253803  117c9dc515                         Set It Up\n",
       "253808  7b3d8d5976  Bring It On: Fight to the Finish\n",
       "253215  f80b7002bb   Anchorman: The Legend Continues\n",
       "253216  771f79dd7e                     The Love Guru"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buckets = lsh(signature_matrix, 4, inverted_users_dict)\n",
    "user_id = \"49d091aa63\"\n",
    "mc_users = most_common_user(user_id, buckets)\n",
    "df_films = get_films(mc_users, movie_genre)\n",
    "df_films.head(5)[[\"movie_id\", \"title\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99b8cd7",
   "metadata": {},
   "source": [
    "## 2. Grouping Users together!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4796fae7",
   "metadata": {},
   "source": [
    "Now, we will deal with clustering algorithms that will provide groups of Netflix users that are similar among them.\n",
    "\n",
    "To solve this task, you must accomplish the following stages:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b03bc8",
   "metadata": {},
   "source": [
    "### 2.1 Getting your data + feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0926b646",
   "metadata": {},
   "source": [
    "1. Access to the data found in this dataset\n",
    "\n",
    "2. Sometimes, the features (variables, fields) are not given in a dataset but can be created from it; this is known as feature engineering. For example, the original dataset has several clicks done by the same user, so grouping data by user_id will allow you to create new features for each user:\n",
    "\n",
    "a) Favorite genre (i.e., the genre on which the user spent the most time)\n",
    "\n",
    "b) Average click duration\n",
    "\n",
    "c) Time of the day (Morning/Afternoon/Night) when the user spends the most time on the platform (the time spent is tracked through the duration of the clicks)\n",
    "\n",
    "d) Is the user an old movie lover, or is he into more recent stuff (content released after 2010)?\n",
    "\n",
    "e) Average time spent a day by the user (considering only the days he logs in)\n",
    "\n",
    "So, in the end, you should have for each user_id five features.\n",
    "\n",
    "3. Consider at least 10 additional features that can be generated for each user_id (you can use chatGPT or other LLM tools for suggesting features to create). Describe each of them and add them to the previous dataset you made (the one with five features). In the end, you should have for each user at least 15 features (5 recommended + 10 suggested by you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5410d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85dd43",
   "metadata": {},
   "source": [
    "### 2.2 Choose your features (variables)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486ae439",
   "metadata": {},
   "source": [
    "You may notice that you have plenty of features to work with now. So, it would be best to find a way to reduce the dimensionality (reduce the number of variables to work with). You can follow the subsequent directions to achieve it:\n",
    "\n",
    "1. To normalise or not to normalise? That's the question. Sometimes, it is worth normalizing (scaling) the features. Explain if it is a good idea to perform any normalization method. If you think the normalization should be used, apply it to your data (look at the available normalization functions in the scikit-learn library).\n",
    "\n",
    "2. Select one method for dimensionality reduction and apply it to your data. Some suggestions are Principal Component Analysis, Multiple Correspondence Analysis, Singular Value Decomposition, Factor Analysis for Mixed Data, Two-Steps clustering. Make sure that the method you choose applies to the features you have or modify your data to be able to use it. Explain why you chose that method and the limitations it may have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87b28d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0ab779",
   "metadata": {},
   "source": [
    "### Clustering!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea10facd",
   "metadata": {},
   "source": [
    "1. Implement the K-means clustering algorithm (not ++: random initialization) using MapReduce. We ask you to write the algorithm from scratch following what you learned in class.\n",
    "\n",
    "2. Find an optimal number of clusters. Use at least two different methods. If your algorithms provide diverse optimal K's, select one of them and explain why you chose it.\n",
    "\n",
    "3. Run the algorithm on the data obtained from the dimensionality reduction.\n",
    "\n",
    "4. Implement K-means++ from scratch and explain the differences with the results you got earlier.\n",
    "\n",
    "5. Ask ChatGPT to recommend other clustering algorithms and choose one. Explain your choice, then ask ChatGPT to implement it or use already implemented versions (e.g., the one provided in the scikit-learn library) and run it on your data. Explain the differences (if there are any) in the results. Which one is the best, in your opinion, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "649b186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9213763",
   "metadata": {},
   "source": [
    "### 2.4 Analysing your results! --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccf18c5",
   "metadata": {},
   "source": [
    "You are often encouraged to explain the main characteristics that your clusters have. The latter is called the Characterizing Clusters step. Thus, follow the next steps to do it:\n",
    "\n",
    "1. Select 2-3 variables you think are relevant to identify the cluster of the customer. For example, Time_Day, Average Click Duration, etc.\n",
    "\n",
    "2. Most of your selected variables will be numerical (continuous or discrete), then categorize them into four categories.\n",
    "\n",
    "3. With the selected variables, perform pivot tables. On the horizontal axis, you will have the clusters, and on the vertical axis, you will have the categories of each variable. Notice that you have to do one pivot table per variable.\n",
    "\n",
    "4. Calculate the percentage by column for each pivot table. The sum of each row (cluster) must be 100. The sample example for clustering with K = 4 and Time_Day variable:\n",
    "\n",
    "| Time_Day | Afternoon | Morning | Night |\n",
    "|----------|-----------|---------|-------|\n",
    "| Cluster|          |       |      |\n",
    "| 1| 3         | 94      | 3     |\n",
    "| 2| 83        | 5       | 12    |\n",
    "| 3| 16        | 10      | 74    |\n",
    "| 4| 34        | 18      | 48    |\n",
    "\n",
    "\n",
    "5. Interpret the results for each pivot table.\n",
    "\n",
    "6. Use any known metrics to estimate clustering algorithm performance (how good are the clusters you found?). Comment on the results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a535f6",
   "metadata": {},
   "source": [
    "## 3. Bonus Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2a3777",
   "metadata": {},
   "source": [
    "We remind you that we consider and grade the bonuses only if you complete the entire assignment.\n",
    "\n",
    "Density-based clustering identifies clusters as regions in the data space with high point density that are separated from other clusters by regions of low point density. The data points in the separating regions of low point density are typically considered noise or outliers. Typical algorithms that fall into this category are OPTICS and DBSCAN.\n",
    "\n",
    "1. Ask ChatGPT (or any other LLM tool) to list three algorithms for Density-Based Clustering. Choose one and use it on the same dataset you used in 2.3. Analyze your results: how different are they from the centroid-based version?\n",
    "\n",
    "Note: You can implement your algorithm from scratch or use the one implemented in the scikit-learn library; the choice is up to you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35169fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162046cd",
   "metadata": {},
   "source": [
    "## 4. Command Line Question (CLQ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7338df",
   "metadata": {},
   "source": [
    "Here is another command line question to enjoy. We previously stated that using the command line tools is a skill that Data Scientists must master.\n",
    "\n",
    "In this question, you should use any command line tool that you know to answer the following questions using the same dataset that you have been using so far:\n",
    "\n",
    "- What is the most-watched Netflix title?\n",
    "- Report the average time between subsequent clicks on Netflix.com\n",
    "- Provide the ID of the user that has spent the most time on Netflix\n",
    "\n",
    "Important note: You may work on this question in any environment (AWS, your PC command line, Jupyter notebook, etc.), but the final script must be placed in CommandLine.sh, which must be executable. Please run the script and include a screenshot of the output in the notebook for evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8dac63d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0b626a",
   "metadata": {},
   "source": [
    "## 5. Algorithmic Question (AQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e525e7c6",
   "metadata": {},
   "source": [
    "Federico studies in a demanding university where he has to take a certain number N of exams to graduate, but he is free to choose in which order he will take these exams. Federico is panicking since this university is not only one of the toughest in the world but also one of the weirdest. His final grade won't depend at all on the mark he gets in these courses: there's a precise evaluation system.\n",
    "\n",
    "He was given an initial personal score of S when he enrolled, which changes every time he takes an exam: now comes the crazy part. He soon discovered that every of the N exams he has to take is assigned a mark p. Once he has chosen an exam, his score becomes equal to the mark p, and at the same time, the scoring system changes:\n",
    "\n",
    "- If he takes an \"easy\" exam (the score of the exam being less than his score), every other exam's mark is increased by the quantity S - p\n",
    "\n",
    "- If he takes a \"hard\" exam (the score of the exam is greater than his score), every other exam's mark is decreased by the quantity p - S\n",
    "\n",
    "So, for example, consider S = 8 as the initial personal score. Federico must decide which exam he wants to take, being [5, 7, 1] the marks list. If he takes the first one, being 5 < 8 and 8 - 5 = 3, the remaining list now becomes [10, 4], and his score is updated as S = .\n",
    "\n",
    "In this chaotic university where the only real exam seems to be choosing the best way to take exams, you are the poor student advisor who is facing a long queue of confused people who need some help. Federico is next in line, and he comes up in turn with an inescapable question: he wants to know which is the highest score possible he could get.\n",
    "\n",
    "a) Fortunately, you have a computer app designed by a brilliant student. Federico wants you to show him the code which this app is based on because he wants to do paid counseling for other desperate students: in a recursive fashion, the helped helps the helpable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "128392f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "13 27 41 59 28 33 39 19 52 48 55 79\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "from itertools import zip_longest\n",
    "#initial personal score\n",
    "S=int(input())\n",
    "# marks\n",
    "exams_grades=list(map(int,input().split()))\n",
    "exams_grades.sort()\n",
    "#we create 2 list where we put the greatest and smallest numbers\n",
    "l1=exams_grades[:len(exams_grades)//2]\n",
    "l2=exams_grades[len(exams_grades)//2:]\n",
    "#we create a new list with an element from the list of the greatest numbers and an element from the list of the smallest numbers\n",
    "if len(l2)==len(l1):\n",
    "  new_list = [elemento for val in zip_longest(l1, l2) for elemento in val]\n",
    "else:\n",
    "  new_list = [elemento for val in zip_longest(l2, l1) for elemento in val if elemento is not None]\n",
    "\n",
    "# we update the marks according to the rule given by the university\n",
    "for x in range(len(new_list)):\n",
    "  grade=new_list[x]\n",
    "  diff=S-grade\n",
    "  for y in range(x+1,len(new_list)):\n",
    "      new_list[y]+=diff\n",
    "  S=grade\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde478c",
   "metadata": {},
   "source": [
    "b) Federico is getting angry because he claims that your code is slow! Show him formally with a big-O notation that he is as crazy as this university!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b5f2f",
   "metadata": {},
   "source": [
    "### Time Complexity Analysis\n",
    "\n",
    "\n",
    "**Input Reading**:\n",
    "Reading\n",
    "S and\n",
    "exams_grades\n",
    "exams_grades takes constant time, so the complexity is O(1).\n",
    "\n",
    "**List Sorting**: Sorting the list\n",
    "exams_grades has a time complexity of\n",
    "O(nlogn), where\n",
    "n is the length of exams_grades.\n",
    "\n",
    "**Creating new_list** : The creation of new_list is performed once and has a linear time complexity relative to the length of the list. Therefore, it is\n",
    "O(n).\n",
    "\n",
    "**Updating Marks**: The time complexity analysis for the updating marks part is as follows:\n",
    "\n",
    "- The outer loop runs N times.\n",
    "- The inner loop also runs, on average, N/2 times across all iterations of the outer loop.\n",
    "- Inside the inner loop, updating the elements of `new_list` takes constant time, O(1).\n",
    "- Therefore, the total time complexity for this part is O(N^2).\n",
    "\n",
    "**In conclusion** : Considering both factors, the overall time complexity of the provided code is dominated by the quadratic term, resulting in a final time complexity of O(N^2) due to the updating marks part.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdb4e14",
   "metadata": {},
   "source": [
    "c) If, unfortunately, Federico is right in the grip of madness, he will threaten you to optimize the code through a different approach. You should end this theater of the absurd by any means! (And again, formally prove that you improved time complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1c42657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "13 27 41 59 28 33 39 19 52 48 55 79\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "def best_grade_improved(S=int(input()),exams_grades=list(map(int, input().split()))):\n",
    "    exams_grades.sort()\n",
    "    n = len(exams_grades)\n",
    "    l1 = exams_grades[:n//2]\n",
    "    l2 = exams_grades[n//2:]\n",
    "# if the number of the exams is even or odd we have to create the list in a different way\n",
    "    if n % 2 == 0:\n",
    "        new_list = [elemento for val in zip_longest(l1, l2) for elemento in val]\n",
    "    else:\n",
    "        new_list = [elemento for val in zip_longest(l2, l1) for elemento in val if elemento is not None]\n",
    "    # after have put the marks in a proper way we call the recursive function to find the best final grade\n",
    "    adjust_grades_recursive(S, new_list)\n",
    "\n",
    "\n",
    "def adjust_grades_recursive(S, exams_grades_systemed):\n",
    "    if not exams_grades_systemed:\n",
    "        print(S)\n",
    "        return\n",
    "    grade = exams_grades_systemed[0]\n",
    "    diff=S-grade\n",
    "    # we update the marks according to the rule given by the university\n",
    "    for y in range(1, len(exams_grades_systemed)):\n",
    "      exams_grades_systemed[y] += diff\n",
    "    S = grade\n",
    "    adjust_grades_recursive(S, exams_grades_systemed[1:])\n",
    "\n",
    "\n",
    "best_grade_improved()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e84ba",
   "metadata": {},
   "source": [
    "### Time Complexity Analysis\n",
    "\n",
    "1. **Input Reading and List Sorting:**\n",
    "   - Reading the personal score `S` and creating the `exams_grades` list take O(N) time.\n",
    "   - Sorting the `exams_grades` list takes O(N log N) time.\n",
    "\n",
    "2. **Creating Lists `l1` and `l2`:**\n",
    "   - Creating `l1` and `l2` by slicing the sorted list takes O(N) time.\n",
    "\n",
    "3. **Creating `new_list`:**\n",
    "   - Creating `new_list` using `zip_longest` and list comprehensions takes O(N) time.\n",
    "\n",
    "4. **Recursive Grade Adjustment:**\n",
    "   - The recursive function `adjust_grades_recursive` processes each element of `exams_grades_systemed` once.\n",
    "   - In each iteration, the function performs constant-time operations (O(1)).\n",
    "   - In the worst case, the recursion depth is N.\n",
    "   - Therefore, the total time complexity for this part is O(N).\n",
    "\n",
    "Considering the sorting operation, the overall time complexity of the improved code is O(N log N) due to the dominant sorting step.\n",
    "\n",
    "So this is more efficient because to update marks has a complexity of O(N) and not O(N^2) as in the former version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1f5956",
   "metadata": {},
   "source": [
    "d) Ask chatGPT for a third (optimized) implementation and analyze again its time complexity. Be careful (and crafty) in defining the prompt, and challenge the machine in this coding question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60787995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "13 27 41 59 28 33 39 19 52 48 55 79\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "from statistics import median\n",
    "def best_grade_improved_chatgpt(S=int(input()), exams_grades=list(map(int, input().split()))):\n",
    "  # we use the median to split the exams_grades in two list one with the largest numbers and the other one with the smallest number\n",
    "    median_value = median(exams_grades)\n",
    "\n",
    "    l1 = [element for element in exams_grades if element < median_value]\n",
    "\n",
    "    l2 = [element for element in exams_grades if element >= median_value]\n",
    "\n",
    "    if len(l2)==len(l1):\n",
    "        new_list = [elemento for val in zip_longest(l1, l2) for elemento in val]\n",
    "    else:\n",
    "        new_list = [elemento for val in zip_longest(l2, l1) for elemento in val if elemento is not None]\n",
    "\n",
    "    adjust_grades_recursive(S, new_list)\n",
    "\n",
    "def adjust_grades_recursive(S, exams_grades_systemed):\n",
    "    if not exams_grades_systemed:\n",
    "        print(S)\n",
    "        return\n",
    "    grade = exams_grades_systemed[0]\n",
    "    diff = S - grade\n",
    "    for y in range(1, len(exams_grades_systemed)):\n",
    "        exams_grades_systemed[y] += diff\n",
    "    S = grade\n",
    "    adjust_grades_recursive(S, exams_grades_systemed[1:])\n",
    "\n",
    "best_grade_improved_chatgpt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc637f",
   "metadata": {},
   "source": [
    "## Time Complexity Analysis\n",
    "\n",
    "### Input Reading and Median Computation:\n",
    "\n",
    "- Reading the personal score `S` and creating the `exams_grades` list take O(N) time.\n",
    "- Computing the median using the `statistics.median` function has an expected time complexity of O(N) but in the worst case is O(N log N) so we have an improvement only on average.\n",
    "\n",
    "### Creating Lists `l1` and `l2`:\n",
    "\n",
    "- Creating `l1` and `l2` using list comprehensions takes O(N) time.\n",
    "\n",
    "### Creating `new_list`:\n",
    "\n",
    "- Creating `new_list` using `zip_longest` and list comprehensions takes O(N) time.\n",
    "\n",
    "### Recursive Grade Adjustment:\n",
    "\n",
    "- The recursive function `adjust_grades_recursive` processes each element of `exams_grades_systemed` once.\n",
    "- In each iteration, the function performs constant-time operations (O(1)).\n",
    "- In the worst case, the recursion depth is N.\n",
    "- Therefore, the total time complexity for this part is O(N).\n",
    "\n",
    "### Overall Time Complexity:\n",
    "\n",
    "Considering all the steps, the overall time complexity of the provided code is O(N log N). The dominant factor is the `statistics.median` function contributing to the overall complexity, we have an improvement only in the average case because `statistics.median`has a complexity of O(N)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921c906e",
   "metadata": {},
   "source": [
    "Here are some input/output examples (the first value is the initial personal score, and the second line contains the list of marks):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564df874",
   "metadata": {},
   "source": [
    "#### Input 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b511c6",
   "metadata": {},
   "source": [
    "8\n",
    "\n",
    "5 7 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375818d5",
   "metadata": {},
   "source": [
    "#### Output 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c8b53",
   "metadata": {},
   "source": [
    "11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d748f07",
   "metadata": {},
   "source": [
    "#### Input 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f07b9a",
   "metadata": {},
   "source": [
    "25\n",
    "\n",
    "18 24 21 32 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89de3ee4",
   "metadata": {},
   "source": [
    "#### Output 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b502751",
   "metadata": {},
   "source": [
    "44"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b450f38",
   "metadata": {},
   "source": [
    "#### Input 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcbaeb1",
   "metadata": {},
   "source": [
    "30\n",
    "\n",
    "13 27 41 59 28 33 39 19 52 48 55 79"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be2881f",
   "metadata": {},
   "source": [
    "#### Output 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb035e4",
   "metadata": {},
   "source": [
    "205"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
